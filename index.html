 <!DOCTYPE html>
<html>
<head>
<title>Advances in Applied Computer Science Invited Speaker Series</title>
<style>
body {
  font-family: 'Roboto Mono';
}
a:link {
  color: white;
  background-color: transparent;
  text-decoration: none;
}
a:visited {
  color:white;
  background-color: transparent;
  text-decoration: none;
}
a:hover {
  color: white;
  background-color: transparent;
  text-decoration: underline;
}
a:active {
  color: white;
  background-color: transparent;
  text-decoration: underline;
}
li::marker {
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Mono">
</head>
<body style="background-color:black;color:white;">

<h1>Advances in Applied Computer Science Invited Speaker Series</h1>

In applied computer science new programming languages and paradigms, like Rust and Julia, new architectures, like Arm and RISC-V, and new GPU’s, like NVidia’s Grace Hopper, AMD's MI300A or Intel Max, are emerging. To keep on top of these advances, the Advances in Applied Computer Science series will invite speakers from academia, industry, and national labs. This will allow researchers and code developers to learn about these new tools and technologies and integrate them into codes to prepare them for future hardware and software.
<p>
Host: Patrick Diehl <a href="https://linkedin.com/in/patrickdiehl"><i class="fa fa-linkedin-square" style="font-size:18px"></i> <a href="https://orcid.org/0000-0003-3922-8419"><i class="ai ai-orcid-square" style="font-size:18px"></i></a></a><br>
Co-Host: Philipp Edelmann <a href="https://orcid.org/0000-0001-7019-9578"><i class="ai ai-orcid-square ai-2x" style="font-size:18px"></i></a><br>

<h2>Overview</h2>

<table style="width:90%">
  <tr>
      <td><b>Speaker</b></td>
      <td><b>Title</b></td>
      <td><b>Date</b></td>
  </tr>
  <tr>
      <td>Alejandra Castillo</td>
      <td><a href="#fy275">Randomized Methods for Corrupted Tensor Systems</a></td>
      <td>12/03/2025</td>
  </tr>
  <tr>
      <td>Chen Zhang</td>
      <td><a href="#fy273">From Anaconda to Pixi: Modernizing Python Package Management for Neutron Science at ORNL</a></td>
      <td>11/12/2025</td>
  </tr>
  <tr>
    <td>Abdel-Hameed (Hameed) A. Badawy</td>
    <td><a href="#fy274">PPT-GPU 2.0: An Accurate, Scalable, Hybrid Performance Model for Modern GPGPUs</a></td>
    <td>10/29/2025</td>
  </tr>
  <tr>
      <td>Rene Gassmoeller</td>
      <td><a href="#fy272">What We Need to Model Planetary Interiors: The Role of Research Software in Computational Geodynamics</a></td>
      <td>10/15/2025</td>
  </tr>
  <tr>
      <td>CJ Solomon</td>
      <td><a href="#fy271">To Interoperability And Beyond: Interoperable Types Through the Promises of C and C++ and ABI Abuse
</a></td>
      <td>10/02/2025</td>
  </tr>
   <tr>
       <td></td>
       <td><b>Fiscal year 2025</b></td>
       <td></td>
  </tr>
  <tr>
      <td>Maxwell Cole</td>
      <td><a href="#fy267">First digital biophysical model of the entire human cardiovascular system</a></td>
      <td>08/27/2025</td>
  </tr>
  <tr>
      <td>Michael L Norman</td>
      <td><a href="#fy268">Computational cosmology meets AI: the birth of a surrogate model for subscale physics</a></td>
      <td>08/13/2025</td>
  </tr>
  <tr>
      <td>Michael L Norman </td>
      <td><a href="#fy266">Introducing Enzo-E, an extreme scale AMR radiation hydrodynamic cosmology code built on Charm++</a></td>
      <td>07/23/2025</td>
  </tr>
  <tr>
      <td>Tim Burke</td>
      <td><a href="#fy265">AI for code development at LANL</a></td>
      <td>07/09/2025</td>
  </tr>
  <tr>
      <td>Dominic Marcello</td>
      <td><a href="#fy262">Shedding Light on Interaction Binaries: Radiation Hydrodynamics with Octo-Tiger</a></td>
      <td>06/25/2025</td>
  </tr>
  <tr>
      <td>Engin Kayraklioglu</td>
      <td><a href="#fy264">Chapel's Batteries-Included Approach for Portable Parallel Programming</a></td>
      <td>06/18/2025</td>
  </tr>
  <tr>
      <td>Jiakun Yan</td>
      <td><a href="#fy258">LCI: a Lightweight Communication Interface for Asynchronous Multithreaded Communication</a></td>
      <td>05/27/2025</td>
  </tr>
  <tr>
      <td>Monica Van Dieren</td>
      <td><a href="#fy263">Accelerated Quantum Supercomputing with CUDA-Q</a></td>
      <td>05/21/2025</td>
  </tr>
  <tr>
      <td>Aparna Chandramowlishwaran</td>
      <td><a href="#fy261">Scaling AI for Scientific Discovery: Faster Kernels, Efficient Models, Better Physics</a></td>
      <td>04/15/2025</td>
  </tr>
  <tr>
      <td>Richard Berger</td>
      <td><a href="#fy260">Driving Continuous Integration and Developer Workflows with Spack</a></td>
      <td>04/08/2025</td>
  </tr>
  <tr>
      <td>Ryan Friese</td>
      <td><a href="#fy259">Introducing the Lamellar Runtime: A Modern Approach to High-Performance Computing</a></td>
      <td>04/02/2025</td>
  </tr>
  <tr>
      <td>Johannes Blaschke</td>
      <td><a href="#fy257">Deploying and Supporting Julia at NERSC</a></td>
      <td>03/12/2025</td>
  </tr>
  <tr>
      <td>William Godoy</td>
      <td><a href="#fy256">The value proposition of the Julia language for DOE’s mission</a></td>
      <td>03/12/2025</td>
  </tr>
  <tr>
      <td>Jonas Posner</td>
      <td><a href="#fy255">Transparent Resource Adaptivity for Task-Based Applications on Supercomputers</a></td>
      <td>02/24/2025</td>
  </tr>
  <tr>
      <td>Jed Brown</td>
      <td><a href="#fy254">Rust and type safety for MPI and scientific computing</a></td>
      <td>02/12/2025</td>
  </tr>
  <tr>
      <td>Alexander Holas</td>
      <td><a href="#fy253">Thermonuclear electron-capture supernovae - Motivating a long-overdue update to the supernova modeling pipeline for the exascale computing age</a></td>
      <td>01/09/2025</td>
  </tr>
  <tr>
      <td>Gregor Dai&szlig;</td>
      <td><a href="#fy252">Asynchronous-Many-Task Systems: Challenges and Opportunities - Scaling an AMR Astrophysics Code on Exascale machines using Kokkos and HPX</a></td>
      <td>12/11/2024</td>
  </tr>
  <tr>
      <td>Chris Taylor</td>
      <td><a href="#fy251">RISC-V HPC Terrain Familiarization</a></td>
      <td>12/04/2024</td>
  </tr>
   <tr>
       <td></td>
       <td><b>Fiscal year 2024</b></td>
       <td></td>
  </tr>
  <tr>
      <td>Kyle Niemeyer</td>
      <td><a href="#fy242">Journal of Open Source Software: bot-assisted open peer review and publication</a></td>
      <td>09/19/2024</td>
  </tr>
  <tr>
      <td>Joseph Schuchart</td>
      <td><a href="#fy241">Improving MPI Interoperability for Modern Languages and Systems</a></td>
      <td>08/21/2024</td>
  </tr>
</table>

<h2>Talks</h2>

<div id="fy275"></div>
<h3>Randomized Methods for Corrupted Tensor Systems</h3>
Speaker: Alejandra Castillo<br>
Pomona College<br><br>

Abstract:
<p>
Recovering tensor-valued signals from corrupted measurements is a central problem in various applications such as hyperspectral image reconstruction and medical imaging. This talk considers tensor linear systems of the form AX = B, that contain observations potentially affected by sparse, large-magnitude corruptions.  A quantile-based randomized Kaczmarz algorithm, called quantile tensor randomized Kaczmarz (QTRK), is discussed to address this challenge. By integrating quantile statistics into the iterative update process, QTRK improves robustness against adversarial errors. A variant selectively omits unreliable measurements to enhance stability further.
</p>

Bio:
<p>
Alejandra Castillo teaches mathematics and statistics at Pomona College. As a statistician, she is interested in applying statistical methodology to questions in biology and public health. Through more recent work, she has collaborated with applied mathematicians to extend linear-algebraic techniques to develop machine learning tools that promote transparency and efficiency in large-scale problems. She introduces students to computing and data science, and also supports efforts to broaden participation in the mathematical sciences.
</p>

<div id="fy273"></div>
<h3>From Anaconda to Pixi: Modernizing Python Package Management for Neutron Science at ORNL</h3>
Speaker: Chen Zhang<br>
Oak Ridge National Laboratory<br><br>

Abstract:
<p>
The neutron science community at Oak Ridge National Laboratory faces unique challenges in managing complex software dependencies across diverse computing environments, from development workstations to production deployment at the Spallation Neutron Source (SNS) and High Flux Isotope Reactor (HFIR). Following Anaconda's licensing changes and mounting technical limitations, we undertook a comprehensive evaluation of modern Python package management solutions to support our neutron data reduction infrastructure.
This talk presents our journey from Anaconda to Pixi, detailing the selection criteria, evaluation process, and practical implementation across our software ecosystem. I will discuss how we assessed alternatives including uv, Poetry, PDM, and micromamba, and why Pixi ultimately met our requirements for reproducibility, performance, and cross-platform compatibility. Through concrete examples from our neutron data projects, I will demonstrate Pixi-based workflows for both pure Python and mixed Python/C++ projects, including our open-source template repository for standardized project setup.
The presentation will cover the complete software lifecycle: from development and testing to packaging for PyPI and conda-forge, and finally to production deployment supporting real-time neutron data reduction at ORNL's flagship facilities. I will share lessons learned, best practices, and practical strategies that may benefit other scientific computing teams facing similar infrastructure modernization challenges.
</p>

Bio:
<p>
Dr. Chen Zhang is a Computational Scientist in the Computer Science and Mathematics Division at Oak Ridge National Laboratory (ORNL), where he specializes in scientific software development, machine learning applications, and high-performance computing for neutron science. He received his B.S. in Materials Science and Engineering from Shanghai Jiao Tong University and his Ph.D. in Materials Science and Engineering from Michigan State University.
Prior to joining ORNL, Dr. Zhang held postdoctoral positions at Carnegie Mellon University and Colorado School of Mines, where he developed automated data reduction software and real-time control systems for high-throughput X-ray diffraction experiments at Argonne National Laboratory's Advanced Photon Source.
At ORNL, Dr. Zhang contributes to software infrastructure development for neutron scattering facilities at the Spallation Neutron Source (SNS) and High Flux Isotope Reactor (HFIR), with a focus on neutron imaging applications. His work includes developing BM3DORNL, an open-source library for artifact removal in neutron tomography; creating FuGNN (Fusion Graph Neural Network) for applications in neutron reflectometry and power-grid analysis; leading the refactoring effort for PLEIADES v2.0, a neutron resonance data reduction software originally developed at Los Alamos National Laboratory; and contributing to iBeatles, the data reduction engine for neutron Bragg edge strain mapping at ORNL's VENUS beamline, where he addressed packaging challenges and implemented command-line batch processing capabilities. He is an active contributor to open-source scientific computing projects and serves as a reviewer for several journals in computational materials science and neutron scattering.
</p>

<div id="fy274"></div>
<h3>PPT-GPU 2.0: An Accurate, Scalable, Hybrid Performance Model for Modern GPGPUs</h3>
Speaker: Abdel-Hameed (Hameed) A. Badawy<br>
New Mexico State University<br><br>

Abstract:
<p>
As modern GPU architectures advance to meet the demands of large-scale simulations and AI, accurate and scalable modeling tools are indispensable for performance optimization and design space exploration. This talk introduces PPT-GPU 2.0, a hybrid performance prediction toolkit designed to model NVIDIA GPUs.
We will detail our enhancements to PPT-GPU, including advanced modeling for the Ampere architecture's redesigned shared memory, local memory, and tensor cores. Additionally, we introduce a new “balance model” to achieve more precise occupancy modeling, complementing metrics such as kernel execution time and active cycles. Furthermore, we integrated a roofline analysis framework directly within PPT-GPU 2.0. This visualization tool enables rapid identification of computational and memory bottlenecks, adding versatility to PPT-GPU 2.0 for scalable hardware-software co-design.
Our evaluations on A100 GPUs are validated against NVIDIA Nsight Compute (Ground Truth) and AccelSim (a cycle-accurate simulator), showing average errors of 16.4% for active cycles and 19.2% for kernel execution time.
Towards the end, time permitting, we will give a glimpse into our hardware security work, which includes using various ML techniques to insert and detect hardware trojans in digital systems.
</p>

Bio:
<p>
Prof. Abdel-Hameed (Hameed) A. Badawy (Senior Member, IEEE) received a B.Sc. degree (Hons.) in Electronics Engineering from Mansoura University, Egypt, focusing on computers and control systems, and an M.Sc. & Ph.D. degrees in computer engineering from the University of Maryland, College Park. He is a tenured Associate Professor with the Klipsch School of Electrical and Computer Engineering, New Mexico State University, Las Cruces, NM, USA. He is the chair of the Computer Engineering Area. Also, he is an affiliated faculty with the Los Alamos and Lawrence Berkeley National Laboratories. He has been a Visiting Research Scientist with the New Mexico Consortium. He was a Lead Research Scientist with the High-Performance Computing Laboratory at the George Washington University. His research interests include performance modeling, prediction, monitoring, and evaluation; high-performance computer architectures; hardware security; machine learning applications to computational problems; and Quantum Computing.
</p>

<div id="fy272"></div>
<h3>What We Need to Model Planetary Interiors: The Role of Research Software in Computational Geodynamics</h3>
Speaker: Rene Gassmoeller<br>
GEOMAR Helmholtz Centre for Ocean Research Kiel, Germany<br><br>

Abstract:
<p>
Geodynamics - understanding the internal dynamics of the Earth and
other planets - depends heavily on the use of numerical algorithms and
their software implementations to make sense of the data we collect
and to understand our exposure to geohazards and the availability of
georesources. The software we use to understand these processes varies
from solvers for wave propagation and elastic deformation over
reactive multi-phase flow models and magneto-hydro-dynamics to
micro-scale mineral physics processes and ab initio material property
prediction. As varied as the software is typically their development
history, and with the advent of modern standards for research software
the geodynamics community decided in 2005 to establish an organization
dedicated to improving software standards - the Computational
Infrastructure for Geodynamics, CIG. 20 years later I will discuss the
role that CIG has played in supporting research software in the field
of computational geodynamics, and the lessons we learned. I will also
present application cases of CIG software, with a particular focus on
the ASPECT software - the Advanced Solver for Planetary Evolution,
Convection, and Tectonics - and how it helps us to understand
deformation processes in Earth's interior, such as the movement of
tectonic plates, and the influence of mineral phase transitions on
Earth's long-term evolution.
</p>

Bio:
<p>
Rene Gassmoeller is a staff scientist at the GEOMAR Helmholtz
Centre for Ocean Research Kiel, and previously was the technical lead
for the NSF-funded community organization Computational Infrastructure
for Geodynamics. He completed his PhD at the German Research Centre
for Geosciences (GFZ) in Potsdam, Germany and did postdoctoral work at
Texas A&amp;M University, Colorado State University and the University of
California, Davis as well as an appointment as visiting assistant
professor and research scientist at the University of Florida. His
research interests lie in computational geodynamics, numerical
mathematics, and software engineering, with a particular focus on
processes in the deep Earth and developing the software we need to
understand them.
</p>

<div id="fy271"></div>
<h3>To Interoperability And Beyond: Interoperable Types Through the Promises of C and C++ and ABI Abuse</h3>
Speaker: CJ Solomon<br>
Los Alamos National Laboratory<br><br>

Abstract:
<p>
Performance portability APIs, e.g., Kokkos, RAJA, etc., are typically written in C++ but many high-performance computing (HPC) codes are written in Fortran.  This presents a challenge for the Fortran simulation codes in being able to shuffle data representations from Fortran to C++, particularly when those data make heavy use of Fortran derived types.  This talk will present a solution developed by LANL’s Eulerian Applications Project (EAP) so that Fortran derived types can be easily migrated from Fortran to C++ and data migrated to the requisite memory spaces to be operated upon by a C++ performance portability library.  The approach builds upon the Fortran 2018 standard “ISO_Fortran_binding.h” header and specialized processing tools to auto generate interoperable types that, in C++, have introspective capability.  The introspective capability of the resulting types allow for transformations of internal data via user defined transformation functions, e.g., to convert Fortran assumed shape arrays into Kokkos::Views
</p>
Links:
<ul>
    <li><a href="https://github.com/lanl/dopey">Github</a> </li>
    <li><a href="https://www.osti.gov/biblio/2372657">Slides</a></li> 
</ul>

<div id="fy267"></div>
<h3>First digital biophysical model of the entire human cardiovascular system</h3>
Speaker: Maxwell Cole<br>
University of California, San Diego<br><br>

Abstract:
<p>
Cardiovascular disease is the leading cause of death worldwide. While substantial progress
has been made in understanding and managing these diseases, current strategies have not
been sufficient to reverse increasing incidence and burden. A potential research solution is
the cardiovascular digital twin, a virtual replica of the human circulatory system. However,
a digital twin of the entire human vasculature has never been accomplished due to the large
computational costs. The goal of this work was to determine the feasibility of a CVDT that
includes modeling all vessels in the human body, including physiologically-relevant biophysical
mechanisms. We used a fractal algorithm to generate all 34 billion blood vessels of the
human body, and calculated the time-dependent blood flow using an integrated heart model.
We included nitric-oxide-mediated vasodilation, as well as vessel deformation and rupture
using peridynamics. To test the computational feasibility, we determined the complexity,
parallel scalability, and the amount of resources required, including execution time, memory
usage, and floating-point operations. We found the CVDT to be computationally feasible,
with all simulations requiring fewer than 30 minutes of wall-clock time. With further computational
optimizations and biophysical improvements, this model has potential to shift
the change the paradigm of cardiovascular research and patient care.
</p>

Bio:
<p>
Dr. Maxwell Cole recently earned his Ph.D. in physics from Louisiana State University,
where he worked on developing the first digital twin of the entire human cardiovascular
system. His research utilized high-performance computing to simulate biophysical processes
at a systemic scale, aiming to create new computational tools that illuminate how diseases
develop and localize in the body. Dr. Cole is now a medical physics resident at the University
of California, San Diego, leveraging physics to advance patient care through improved
prevention, diagnosis, and treatment.
</p>

<div id="fy268"></div>
<h3>Computational cosmology meets AI: the birth of a surrogate model for subscale physics</h3>
Speaker: Michael L Norman<br>
Distinguished Professor of Astronomy and Astrophysics at UC San Diego<br><br>

Abstract:
<p>
This is the story of how my UCSD graduate student (Azton Wells) hijacked the computational cosmology thesis project I gave him and turned it into an integrated ModSim/AI thesis.  Azton wanted to apply what he had learned about machine learning in CS classes to the problem I suggested. The question was: How do the first stars in the universe pollute the universe with heavy elements and how does that affect the properties of the first galaxies forming subsequently? The product of his thesis research is StarNet [1,2,3], a deep learning based surrogate model for primordial star formation and feedback. In this talk I trace how the surrogate model emerged through trial and error, and how the model was trained using data produced by resolved numerical simulations of primordial star formation and feedback in protogalaxies. I discuss the crucial differences between resolved scales, unresolved scales, and inference scales in the context of our model. I then describe how the surrogate model, prototyped in PyTorch, was exported so that it can be used for inference in running Enzo and Enzo-E simulations. I touch on inference mesh data structures incorporated into Enzo-E for this purpose. I then show some results of simulations using StarNet of the formation of the first galaxies in the universe.
</p>

Bio:
<p>
Michael L Norman is Distinguished Professor of Astronomy and Astrophysics at UC San Diego and former director of the San Diego Supercomputer Center. He received his PhD in Engineering and Applied Science from UC Davis while working at the Lawrence Livermore National Laboratory. Subsequently he held research positions at the Max Planck Institute for Astrophysics, Los Alamos National Laboratory, and National Center for Supercomputing Applications, UIUC. His research group, the Laboratory for Computational Astrophysics (est. 1991) develops community application software for astrophysical simulation on supercomputers including ZEUS-2D, ZEUS-3D, ZEUS-MP, Enzo, and Enzo-E. His scientific interests include astrophysical and cosmological fuid dynamics with applications to star formation, interstellar medium, supernova remnants, astrophysical jets, galaxy formation, and X-ray clusters. His technical interests include algorithm development, code development, and parallel computing. In his role as SDSC director, he served as PI for the Gordon, Comet, and Expanse national HPC systems and is currently PI of the CloudBank cloud access project funded by the NSF.
</p>

References:
<ol>
    <li>Wells, A. I. & Norman, M. L. 2021; Predicting Localized Primordial Star Formation with Deep Convolutional Neural Networks, doi: 10.3847/1538-4365/abfa17 </li>
    <li>Wells, A. I. & Norman, M. L. 2022a; Connecting Primordial Star-forming Regions and Second-generation Star Formation in the Phoenix Simulations, doi: 10.3847/1538-4357/ac6c87</li>
    <li>Wells, A. I. & Norman, M. L. 2022b; The First Galaxies and the Effect of Heterogeneous Enrichment from Primordial Stars, doi:10.48550/arXiv.2210.14805</li>
</ol>


<div id="fy266"></div>
<h3>Introducing Enzo-E, an extreme scale AMR radiation hydrodynamic cosmology code built on Charm++</h3>
Speaker: Michael L Norman<br>
Distinguished Professor of Astronomy and Astrophysics at UC San Diego<br><br>

Abstract:
<p>
Enzo-E is a new, extreme-scale implementation of the venerable Enzo community code (enzo-project.org) for astrophysics and cosmology simulation. Enzo has been in widespread use since the early 2000’s but was not designed for today’s HPC architectures. Enzo-E achieves its extreme scalability through the adoption of a forest-of-octrees AMR mesh approach and the use of the Charm++ parallel object framework which manages the fully distributed AMR data structure. Charm++ is a message-driven asynchronous many-task framework developed at UIUC (charm.cs.illinois.edu). In this talk I briefly review Enzo-E’s software design, core AMR methods, physics algorithms, and scalability results. Recently, we incorporated a deep learning surrogate model for star formation and feedback which relaxes the resolution requirements in large cosmological simulations of the first galaxies. Current development work is focused on implementing hierarchical adaptive time stepping similar to Enzo’s. Performance studies suggest a 5-10x speedup over the current global timestep approach.
</p>

Bio:
<p>
Michael L Norman is Distinguished Professor of Astronomy and Astrophysics at UC San Diego and former director of the San Diego Supercomputer Center. He received his PhD in Engineering and Applied Science from UC Davis while working at the Lawrence Livermore National Laboratory. Subsequently he held research positions at the Max Planck Institute for Astrophysics, Los Alamos National Laboratory, and National Center for Supercomputing Applications, UIUC. His research group, the Laboratory for Computational Astrophysics (est. 1991) develops community application software for astrophysical simulation on supercomputers including ZEUS-2D, ZEUS-3D, ZEUS-MP, Enzo, and Enzo-E. His scientific interests include astrophysical and cosmological fuid dynamics with applications to star formation, interstellar medium, supernova remnants, astrophysical jets, galaxy formation, and X-ray clusters. His technical interests include algorithm development, code development, and parallel computing. In his role as SDSC director, he served as PI for the Gordon, Comet, and Expanse national HPC systems and is currently PI of the CloudBank cloud access project funded by the NSF.
</p>

<div id="fy265"></div>
<h3>AI for code development at LANL</h3>
Speaker: Tim Burke<br>
Los Alamos National Laboratory<br><br>

Abstract:
<p>
As AI rapidly transforms code development, with industry giants like Microsoft reporting up to 30% of new code being AI-generated, this talk explores the AI resources available to LANL developers. We'll overview the large language models accessible to LANL employees, including ChatGPT Enterprise, AWS-hosted LLMs, and locally-running open-weight models. The presentation will cover both chat UI applications and API integrations for IDEs, Python scripts, and agentic workflows. Real-world examples will demonstrate AI's impact on productivity in LANL code projects, along with insights into emerging agentic workflows. This talk aims to equip LANL developers with practical knowledge of AI tools to enhance their coding efficiency and innovation.
</p>

Bio:
<p>
Tim is the MCATK Monte Carlo radiation transport code team leader in XCP-3. He joined the lab as a Metropolis Postdoctoral Fellow in 2017 after receiving his Ph.D. in Nuclear Engineering and Radiological Sciences and Scientific Computing from the University of Michigan. Tim's work interests focus on productivity–accelerating simulations on GPUs and accelerating code development workflows. He is an early adopter of AI at LANL and continues to push for greater AI resources for code developers.
</p>


<div id="fy262"></div>
<h3>Shedding Light on Interaction Binaries: Radiation Hydrodynamics with Octo-Tiger</h3>
Speaker: Dominic Marcello <a href="https://www.linkedin.com/in/dominic-marcello-71381718b/"><i class="fa fa-linkedin-square" style="font-size:18px"></i></a><br>
Center for Computation & Technology<br>
Louisiana State University<br><br>

Abstract:
<p>
Octo-TIGER is an adaptive, octree-based code for modeling self-gravitating hydrodynamics in interacting binaries. We have extended Octo-TIGER to include gray radiation transport via the M1 moment method, coupling radiation energy and flux to the existing PPM hydrodynamics and FMM gravity solvers within a task-based HPX framework. Radiation variables are discretized on the same AMR mesh and evolved using an implicit-explicit scheme to handle both optically thick and thin regimes while conserving total energy and radiative flux. We validate the RHD module with standard radiation test problems, including a planar radiation front propagating through uniform media, radiative equilibration in matter–radiation coupling tests, and diffusion of a Gaussian radiation pulse. These benchmarks confirm the accuracy and efficiency of the M1 implementation in Octo-TIGER, laying the groundwork for forthcoming production runs of interacting binaries with full radiation feedback.
</p>

Bio:
<p>
Dominic Marcello is the lead developer of the Octo-TIGER project at the Center for Computation & Technology at Louisiana State University. After earning his PhD in Physics in 2011, he served as a postdoctoral researcher in LSU’s Physics Department before joining CCT as a staff researcher. His work focuses on high-performance computational methods for astrophysics, particularly simulating interacting binary systems using adaptive mesh techniques, advanced hydrodynamics solvers, and radiation transport.
</p>

<div id="fy264"></div>
<h3>Chapel's Batteries-Included Approach for Portable Parallel Programming</h3>
Speaker: Engin Kayraklioglu <a href="https://www.linkedin.com/in/engink/"><i class="fa fa-linkedin-square" style="font-size:18px"></i></a><br>
Hewlett Packard Enterprise (HPE)<br><br>

Abstract:
<p>
Virtually all modern processors, including GPUs, from the consumer-grade to the highest-end are parallel. However, none of the mainstream programming languages support different flavors of hardware parallelism as a first-class concept. Capability to exploit parallelism in the hardware is typically achieved by augmenting inherently sequential programming languages like C, C++, or Fortran with add-ons like OpenMP, MPI, CUDA, OpenCL, or Kokkos.<br><br>
Chapel stands out as a programming language designed with parallelism and locality as first-class concepts in the programming language itself. That, and other modern programming features, have enabled Chapel users to overcome the barriers to parallel programming and HPC. In this talk, I will go over how parallelism and locality are expressed in Chapel, and how they can be used to target GPUs in a vendor-neutral manner. I will also refer to several success stories from the Chapel community.
</p>

Bio:
<p>
Engin Kayraklioglu is a Principal Engineer at Hewlett Packard Enterprise (HPE), where he is a member of the Chapel team. His duties include leading GPU support effort, as well as the Arkouda project (<a href="https://arkouda-www.github.io/">https://ark    ouda-www.github.io/</a>). Before joining HPE (formerly Cray Inc.), Engin received his Ph.D. from The George Washington University in 2019. His interests are high-performance computing, programmer productivity and performance optimizations.
</p>
Materials:
<ul>
    <li><a href="https://chapel-lang.org/presentations/EnginLANL2025.pdf">Slides</a></li>
</ul>


<div id="fy263"></div>
<h3>Accelerated Quantum Supercomputing with CUDA-Q</h3>
Speaker: Monica Van Dieren <a href="https://www.linkedin.com/in/monica-vandieren/"><i class="fa fa-linkedin-square" style="fon    t-size:18px"></i></a><br>
Nvidia<br><br>

Abstract:
<p>
Quantum computing holds the promise of transforming industries and ushering in a new era of high-performance computing. However, achieving this potential depends heavily on AI supercomputing to address algorithm research and overcome the challenges of design, control, and integration that impede the development of practical quantum devices. This presentation explores the future landscape of large-scale, commercially viable hybrid quantum devices, enabled by GPU-accelerated simulations, AI-enhanced algorithms, and tightly coupled HPC control systems. We also introduce CUDA-Q, the open source, qubit-agnostic platform driving this vision forward.
</p>

Bio:
<p>
As a Senior Technical Marketing Engineer at NVIDIA, Monica VanDieren specializes in quantum and high-performance computing, driving initiatives such as the CUDA-Q Academic program. Before joining NVIDIA, Monica worked on IBM’s Quantum Accelerator programs. With a Ph.D. in Mathematical Sciences from Carnegie Mellon University, she brings over 20 years of academic experience at universities such as Stanford, University of Michigan, and Robert Morris University to her work.

<div id="fy258"></div>
<h3>LCI: a Lightweight Communication Interface for Asynchronous Multithreaded Communication</h3>
Speaker: Jiakun Yan <a href="https://www.linkedin.com/in/jiakun-yan-81bb88196/"><i class="fa fa-linkedin-square" s    tyle="font-size:18px"></i></a><br>
University of Illinois Urbana-Champaign<br><br>

Abstract:
<p>
The Lightweight Communication Interface (LCI) is an experimental communication library aiming for better asynchronous multithreaded communication support, both in terms of performance and programmability. It is also a research tool helping us understand how to design communication libraries to better fit the needs of dynamic programming systems/applications. It features a simple, incrementally refinable interface unifying all common point-to-point communication primitives and an atomic-based runtime for maximum threading efficiency. It has been integrated into established asynchronous many-task systems such as HPX and shown significant performance improvement on microbenchmarks/real-world applications. This talk will present an overview of its interface and software design and showcase its performance.
</p>

Bio:
<p>
Jiakun Yan is a fifth-year Ph.D. student at UIUC, advised by Prof. Marc Snir. His research involves exploring better communication library designs for highly dynamic/irregular programming systems and applications. He is the main contributor to the Lightweight Communication Interface (LCI) Project and the HPX LCI parcelport.
</p>


<div id="fy261"></div>
<h3>Scaling AI for Scientific Discovery: Faster Kernels, Efficient Models, Better Physics</h3>
Speaker: Aparna Chandramowlishwaran <a href="https://www.linkedin.com/in/aparna-chandramowlishwaran-302153b/"><i class="fa fa-linkedin-square" style="font-size:18px"></i></a> <br>
University of California, Irvine<br><br>

Abstract:
<p>
As AI becomes a powerful tool in scientific computing, two challenges emerge: (1) efficiently scaling models on modern hardware, and (2) ensuring these models can learn complex physics with fidelity and generalizability. In this talk, I will discuss our work at this intersection.
First, I will introduce Fused3S, a GPU-optimized algorithm for sparse attention—the backbone of graph neural networks and transformers. By fusing matrix operations, Fused3S reduces data movement and maximizes tensor core utilization, achieving state-of-the-art performance across diverse workloads. Then, I will present BubbleML and Bubbleformer, our efforts to model boiling dynamics with ML. Boiling is fundamental to energy, aerospace, and nuclear applications, yet remains difficult to model due to the interplay of turbulence, phase change, and nucleation. By combining large-scale simulation datasets with transformer architectures, Bubbleformer forecasts boiling dynamics across fluids, geometries, and operating conditions. Together, these efforts illustrate how scaling AI—both computationally and scientifically—can accelerate discovery across disciplines. I will conclude with open challenges and opportunities in AI-driven scientific computing, from hardware-aware models to foundation models for physics.
</p>

Bio:
<p>
Aparna Chandramowlishwaran is an Associate Professor at the University of California, Irvine, in the Department of Electrical Engineering and Computer Science. She received her Ph.D. in Computational Science and Engineering from Georgia Tech in 2013 and was a research scientist at MIT prior to joining UCI as an Assistant Professor in 2015. Her research lab— HPC Forge—aims at advancing computational science using high-performance computing and machine learning. She currently serves as the associate editor of the ACM Transactions on AI for Science.
</p>


<div id="fy260"></div>
<h3>Driving Continuous Integration and Developer Workflows with Spack</h3>
Speaker: Richard Berger <a href="https://www.linkedin.com/in/richard-felix-berger/"><i class="fa fa-linkedin-square" style="font-size:18px"></i></a><br>
Los Alamos National Laboratory<br><br>

Abstract:
<p>
Spack makes it easy to install dependencies for our software on multiple HPC platforms. However, there is little guidance on how to structure Spack environments for larger projects, share common Spack installations with code teams and utilize them in an effective way for continuous integration and development. This presentation will share some of the lessons learned from deploying chained Spack installations for multiple code teams at LANL on various HPC platforms both on site and on other Tri-Lab systems, how to structure such deployments for reusability and upgradability, and make them deployable even on air-gapped systems. It will also show how we utilize Spack's build facilities to drive CMake-based projects on GitLab for continuous integration, without having to replicate build configuration logic in GitLab files, while giving developers an easy-to-follow workflow for recreating CI runs in various configurations.
</p>

Bio:
<p>
Richard is a research software engineer in the Applied Computer Science Group (CCS-7) at Los Alamos National Laboratory (LANL) with a background in Mechatronics, high-performance computing, and software engineering. He is currently contributing to the core development of LAMMPS, FleCSI and working on DevOps for multiple other LANL projects.
</p>

<div id="fy259"></div>
<h3>Introducing the Lamellar Runtime: A Modern Approach to High-Performance Computing</h3>
Speaker: Ryan Friese <a href="https://www.linkedin.com/in/ryandfriese/"><i class="fa fa-linkedin-square"     style="font-size:18px"></i></a> <br>
Pacific Northwest National Laboratory<br>
Senior computer scientist<br><br>

Abstract:
<p>
In the realm of High-Performance Computing (HPC), achieving peak system performance while maintaining code safety and concurrency has always been a challenging endeavor. Traditional HPC frameworks often struggle with memory safety issues, race conditions, and the complexities of parallel programming. In this talk, I introduce Lamellar, a new HPC runtime that leverages the modern programming language Rust to address these enduring challenges.
Rust, known for its powerful type system and memory safety guarantees without a garbage collector, is rapidly gaining traction within the systems programming community. However, its potential in the HPC domain is yet to be fully explored. The Lamellar runtime harnesses Rust's strengths, providing a robust, scalable, and safe environment for developing and executing high-performance applications.
This talk is designed for computational domain and computer scientists who are well-acquainted with HPC concepts but may be new to Rust. The talk will cover the following key topics:
<ol>
    <li>Introduction to Rust: An overview of Rust’s features and why it's suitable for HPC applications.</li>
    <li>Understanding the Lamellar Runtime: A deep dive into the architecture and core components of the Lamellar runtime and stack.</li>
    <li>Concurrency and Parallelism: How Rust’s concurrency model and Lamellar API’s ensure safe parallel execution, eliminating common pitfalls like data races.</li>
    <li>Performance Benchmarks: Comparative analysis showcasing the performance benefits of using Rust Lamellar Runtime in selected benchmarks.
    </li>
    <li>Getting Started: Resources, tools, and best practices for adopting Rust and Lamellar in your HPC projects.</li>
</ol>
Join me as we look into the future of HPC with Rust and the Lamellar Runtime.
<p>
Bio:
<p>
Dr. Ryan Friese is a senior computer scientist at Pacific Northwest National Laboratory in the Future Computing Technologies group. His research interests span hardware/software co-design of runtime and system software for novel architectures, HPC (high performance computing) network simulation and modeling, the analysis and optimization of data movement in large scale distributed workflows, and performance modeling of irregular applications. His recent work has focused on enabling memory safe programming on HPC systems by leading the development of the Lamellar Runtime, an asynchronous distributed runtime written in the Rust Programming Language. He received his PhD in Electrical & Computer Engineering in 2015 from Colorado State University. 
</p>


<div id="fy257"></div>
<h3>Deploying and Supporting Julia at NERSC</h3>
Speaker: Johannes Blaschke <a href="https://www.linkedin.com/in/johannes-blaschke/"><i class="fa fa-linkedin-square" style="font-size:18px"></i></a><br>
National Energy Research Scientific Computing Center<br>
Application Performance Specialist<br><br>

Abstract:
<p>
Julia has been supported as a first-class language on NERSC's systems for over 10 years [1]. In this talk we will discuss how Julia has been deployed at scale, the technical issues encountered and how they were eventually overcome. We will also explore the challenges faced with developing intuitive and portable distributed HPC applications that are capable of targeting modern GPU architectures, and NERSC's vision for supporting modern interdisciplinary and multi-facility workflows. 
<br>
<br>
[1] <a href="https://info.juliahub.com/case-studies/celest">https://info.juliahub.com/case-studies/celeste</a>
</p>

Bio:
<p>
Johannes Blaschke is a HPC workflow performance expert leading the NERSC Science Acceleration Program (NESAP). His research interests include urgent and interactive HPC; and programming environments and models for cross-facility workflows. Johannes supports Julia on NERSC's systems, including one of the first examples of integrating MPI.jl and Distributed.jl with the HPE's Slingshot network technology. Johannes is a zealous advocate for Julia as an HPC programming language, and a contributor and organizer of Julia tutorials and BoFs at SC, JuliaCon and within the DoE.
</p>

<div id="fy256"></div>
<h3>The value proposition of the Julia language for DOE’s mission</h3>
Speaker: William Godoy <a href="https://www.linkedin.com/in/william-f-godoy-7326a351/"><i class="fa fa-linkedin-square"     style="font-size:18px"></i></a><br>
Oak Ridge National Laboratory<br>
Senior computer scientist<br><br>

Abstract:
<p>
We present a summary of our research and community efforts exploring the Julia
language for the scientific mission of the US Department of Energy (DOE) at the
intersection of high-performance computing (HPC) and high-productivity. Powered by
the LLVM compiler infrastructure combined with a unifying ecosystem and friendly
scientific syntax, Julia attempts to lower cost of a “two-language and multiple
ecosystems” paradigm (e.g. Python+compiled language). Along with the Julia intro and
HPC hands-on tutorials, we present our efforts on: (i) building an accessible
performance portable CPU/GPU library: JACC.jl, (ii) the outcome of external venues
(SC BoFs, tutorials) and workshops at Oak Ridge National Laboratory (ORNL), and (iii)
our research, best paper at SC23 WORKS, on the unifying value for using a single
front-end language on Frontier, the second fastest supercomputer in the world, and (iv)
our work, best paper at SC24 XLOOP, connecting ORNL’s experimental and
computational facilities using JACC.jl. Hence, Julia aspires to make more accessible the
future landscape of heterogeneous, AI-driven, and energy-aware computing by
leveraging existing investments outside DOE in LLVM and commercial applications of
the language.
</p>

Bio:
<p>
William Godoy is a senior computer scientist in the Computer Science and Mathematics
Division at Oak Ridge National Laboratory (ORNL). His interests are in
high-performance computing, parallel programming systems, scientific software and
workflows. At ORNL, he contributed to the Exascale Computing Project applications
-QMCPACK- and software technologies portfolios – ADIOS2, Julia/LLVM, and projects
impacting ORNL’s computing and neutron science facilities. Godoy currently works
across research projects funded by the US Department of Energy Advanced Scientific
Computing Research (ASCR) program. Prior to ORNL, he was a staff member at Intel
Corporation and a postdoctoral fellow at NASA Langley Research Center. Godoy
received PhD and MSc degrees from the University at Buffalo, The State University of
New York, and a BSc from the National Engineering University (UNI) Lima, Peru, in
mechanical engineering. He is a senior member of the IEEE, and a member of ACM,
ASME and US-RSE serving in several venues and technical committees.
</p>

<div id="fy255"></div>
<h3>Transparent Resource Adaptivity for Task-Based Applications on Supercomputers</h3>
Speaker: Jonas Posner <a href="https://www.linkedin.com/in/jonas-posner-086381149/"><i class="fa fa-linkedin-square"     style="font-size:18px"></i></a><br>
University of Kassel, Germany<br>
Substitute Chair - Software Engineering<br><br>

Abstract:
<p>
Traditional static resource allocation in supercomputers (jobs retain a fixed set of resources) leads to inefficiencies. Resource adaptivity (jobs can change resources at runtime) significantly increases supercomputer efficiency.<br>
This talk will exploit Asynchronous Many-Task (AMT) programming, which is well suited for adaptivity due to its transparent resource management. An AMT runtime system dynamically assigns user-defined small tasks to workers to achieve load balancing and adapt to resource changes.<br>
We will discuss techniques for malleability and evolving capabilities that allow programs to dynamically change resources without interrupting computation. Automatic load detection heuristics determine when to start or terminate processes, which is particularly beneficial for unpredictable workloads. Practicality is demonstrated by adapting the GLB library. A generic communication interface allows interaction between programs and resource managers. Evaluations with a prototype resource manager show significant improvements in batch makespan, node utilization, and job turnaround time for both malleable and evolving jobs.
</p>

Bio:
<p>
Jonas is a dedicated computer scientist specializing in High Performance Computing. He received his Bachelor’s and Master’s degrees from the University of Kassel, Germany, where he also earned his Ph.D. in 2022. He is currently working as an substitute chair for the Software Engineering Group at the same university and is also writing his habilitation.<br>
Jonas' research interests include load balancing, fault tolerance, and resource adaptivity for Asynchronous Many-Task (AMT) systems. Recently, he has focused on resource adaptivity in general to optimize the efficient use of supercomputing resources. His work covers a broad spectrum, including the development of advanced job scheduling algorithms, the improvement of application programming using AMT systems, and the interaction between resource managers and jobs.
</p>

<div id="fy254" />
<h3>Rust and type safety for MPI and scientific computing</h3>
Speaker: Jed Brown <a href="https://www.linkedin.com/in/jed-brown-78295421/"><i class="fa fa-linkedin-square"         style="font-size:18px"></i></a><br>
University of Colorado Boulder<br>
Associate Professor of Computer Science<br><br>

Abstract:
<p>
Rust is a modern language that provides type- and memory-safety without a garbage collector, using a concept called lifetimes. Many see Rust as a successor to languages like C and C++, and there are many interested individuals in the computational science community, yet few major projects have made the switch. I'll introduce the language and its ecosystem, including the state of scientific computing libraries. We'll discuss what soundness means for libraries and examine rsmpi, which safely exposes MPI and allows catching many common bugs at compile-time. We'll also discuss type-system approaches to collective semantics, and conclude with an outlook on Rust for scientific computing.
</p>

Bio:
<p>
Jed leads the Physical Prediction, Inference, and Design group at CU Boulder. He is a maintainer of PETSc, libCEED, and rsmpi (Rust bindings to MPI), and is active in many open source communities. He works on high-performance numerical software infrastructure for computational science and engineering, as well as applications such as structural mechanics and materials science, non-Newtonian and turbulent flows, and plasmas. He is co-director of the PSAAP-3 Multidisciplinary Simulation Center for Micromorphic Multiphysics Porous and Particulate Materials Simulations Within Exascale Computing Workflows.
</p>
Materials:
<ul>
    <li><a href="https://github.com/jedbrown/talks/blob/main/20250212-RustSciComp.ipynb">Slides</a></li>
</ul>

<div id="fy253"></div>
<h3>Thermonuclear electron-capture supernovae - Motivating a long-overdue update to the supernova modeling pipeline for the exascale computing age</h3>
Speaker: Alexander Holas<br>
Heidelberg Institute for Theoretical Studies<br><br>

Abstract:
<p>
The thermonuclear supernova modeling pipeline has been refined for over four decades and has achieved substantial success in modeling various supernova subtypes. Nonetheless, continuous innovation is essential for maintaining supernova modeling at the forefront of computational astrophysics. In this work, we examine a novel scenario, so called thermonuclear electron-capture supernovae. Originally proposed by Jones et al. (2016), this scenario consists of a collapsing sAGB star that only narrowly escape collapse to a neutron star by runaway thermonuclear thermonuclear burning. Here, we explore the specific circumstances under which such a thermonuclear explosion can occur and under which conditions the collapse can be averted by nuclear burning. Subsequently, we leverage this scenario to motivate a long-overdue update to the thermonuclear supernova modeling pipeline, both by increasing the complexity of the physics included, as well as updating the underlying codebase for the latest exascale computing clusters. In particular, we advocate the integration of radiation hydrodynamics and the transition towards a performance portable programming model.
</P>

Bio:
<ul>
<li> Undergraduate at Ulm University</li>
<li> Master Studies at the Technical University of Munich and the Max Planck Institute for Astrophysics (Thesis: Determination of the Expansion Rate of the Universe by Means of Type II‑P Supernovae)</li>
<li> PhD at the Heidelberg Institute for Theoretical Studies/ Heidelberg University on numerical simulations of thermonuclear supernovae (Working project title: Thermonuclear electron-capture supernovae - thermonuclear explosion or gravitational collapse?)</li>
<li> Fellow at the International Max Planck Research School Heidelberg</li>
</ul>

<div id="fy251"></div>
<h3>RISC-V HPC Terrain Familiarization</h3>
Speaker: Chris Taylor <a href="https://www.linkedin.com/in/christopher-taylor-8250a48/"><i class="fa fa-linkedin-square"             style="font-size:18px"></i></a><br>
Tactical Computing Lab<br>
Principle Research Engineer<br><br>

Abstract:
<p>
The number of RISC-V commercial products increased substantially this past year. This presentation is an orientation to the range of RISC-V hardware, HPC software support, the community, and the current state of HPC-relevant ISA extensions. Acquiring RISC-V hardware is no longer a question of when - it is possible now.
</p>

Bio:
<p>
Chris is a senior principle research engineer at Tactical Computing Labs. His work experience includes compilers, runtime systems, systems level software, numerical libraries, applied math problems, and hardware simulation. He has a Masters Degree in Computer Science from Georgia Tech and an undergraduate degree in Computer Science from Clemson.
</p>

<div id="fy252"></div>
<h3>Asynchronous-Many-Task Systems: Challenges and Opportunities - Scaling an AMR Astrophysics Code on Exascale machines using Kokkos and HPX</h3>

Speaker: Gregor Dai&szlig; <a href="https://www.linkedin.com/in/gregor-daiss/"><i class="fa fa-linkedin-square"             style="font-size:18px"></i></a><br>
University of Stuttgart<br>
Institute for Parallel and Distributed System<br><br>

Abstract:
<p>
Dynamic and adaptive mesh refinement is pivotal in high-resolution,
multi-physics simulations, necessitating precise physics resolution in
localized areas across expansive domains. Today's supercomputers'
extreme heterogeneity and large number of compute nodes present a
significant challenge for such dynamically adaptive codes, highlighting
the importance of both scalability and performance portability. Our
research focuses on how to address this by integrating the asynchronous,
many-task runtime system HPX with the performance-portability framework
Kokkos and SIMD types. To demonstrate and benchmark our solutions at
scale, we incorporated them into Octo-Tiger, an adaptive, massively
parallel application for the simulation of binary star systems and their
outcomes. Thanks to this, Octo-Tiger now supports a diverse set of
processors, accelerators, as well as network backends and can scale on
various supercomputers, such as Perlmutter, Frontier, and Fugaku. In
this talk, we outline our various integrations between HPX and Kokkos.
Furthermore, we show the challenges we encountered when using these
frameworks together in Octo-Tiger and how we addressed them, ultimately
achieving scalability on a selection of current supercomputers.
</p>

Bio:
<p>
Gregor Daiß is a PhD student at the University of Stuttgart,
specializing in high-performance computing. His main interests
include task-based runtime systems, distributed computing,
performance-portability as well as refactoring large-scale
simulations and porting them to accelerators. Current work
mostly involves both Kokkos (for performance-portability) and
HPX (task-based runtime system) for these purposes.
</p>

<div id="fy242"></div>
<h3>Journal of Open Source Software: bot-assisted open peer review and publication</h3>

Speaker: Kyle Niemeyer <a href="https://www.linkedin.com/in/kyleniemeyer/"><i class="fa fa-linkedin-square"                 style="font-size:18px"></i></a><br>
Oregon State University<br>
School of Mechanical, Industrial, and Manufacturing Engineering<br>
Associate Professor<br><br>

Abstract:
<p>
The Journal of Open Source Software (JOSS) is an open-access, no-fee scholarly journal that publishes quality open-source research software based on open peer review. JOSS was founded in 2016 with the dual objectives of giving traditional academic publication credit for software work and improving the quality of research software. Since its founding, JOSS has published over 2500 software papers—and counting!—with over 80 active editors spread across seven topic-area tracks. To handle this level of submissions and publishing, relying on a fully volunteer team, JOSS relies on GitHub and a system of open tools for reviewing and publishing submissions, driven by chatbot commands. Authors submit short Markdown papers along with links to their software's repository, which are compiled to PDF via Pandoc. JOSS’s editorial bot performs automated health checks on submissions, and reviews take place in GitHub issues, with authors, editors, and reviewers issuing bot commands via comments. This talk will describe the publication experience of JOSS and its machinery, and how it can be adapted by other communities.
</p>

Bio:
<p>
Kyle E. Niemeyer is an Associate Professor at Oregon State University in the School of Mechanical, Industrial, and Manufacturing Engineering. He also serves as the Associate School Head for Undergraduate Programs. He leads the Niemeyer Research Group, which uses computational modeling to study various phenomena involving fluid flows, including combustion and chemical kinetics, and related topics like numerical methods and parallel computing. He is also a strong advocate of open access, open source software, and open science in general, and has contributed in the area of standardizing research software citation. Kyle has received multiple prestigious fellowships throughout his career, including the AAAS Science & Technology Policy Fellowship in 2022, the Better Scientific Software (BSSw) Fellowship in 2019, the NSF Graduate Research Fellowship in 2010, and the National Defense Science and Engineering Graduate Fellowship in 2009. Kyle received his Ph.D. in Mechanical Engineering from Case Western Reserve University in 2013. He received BS and MS degrees in Aerospace Engineering from Case Western Reserve University in 2009 and 2010, respectively.
</p>
Material:
<ul>
    <li> <a href="https://zenodo.org/records/13799976">Slides</a> <i class="ai ai-zenodo ai-2x"></i></li>
</ul>

<div id="fy241"></div>
<h3>Improving MPI Interoperability for Modern Languages and Systems</h3>

Speaker: Joseph Schuchart <a href="https://www.linkedin.com/in/joseph-schuchart-44368077/"><i class="fa fa-linkedin-square"                     style="font-size:18px"></i></a><br>
Stony Brook University<br>
Institute for Advanced Computational Science<br>
Senior Research Scientist<br><br>

Abstract:
<p>
The Message Passing Interface standard has long been the lingua franca of HPC. Its design has enabled the development of many distributed parallel applications. After 30 years, the field of high-performance computing has seen several programming paradigms come and go. However, MPI has yet to address the challenges of accelerator-based computing, the advent of modern languages such as Rust, Python, and C++, and fully asynchronous programming models. This talk will provide insights into current efforts on modernizing MPI, from accelerator integration to improved datatype handling for modern languages.
</p>

Bio:
<p>
Joseph Schuchart is a Senior Research Scientist at the Institute for Advanced Computational Sciences at Stony Brook University. His research revolves around distributed asynchronous and task-based programming models, communication libraries, and design aspects of integrating different models. Joseph received his M.Sc. in Computer Science from Dresden University of Technology in 2012 and his PhD from the University of Stuttgart in 2020. He is an active member of the MPI Forum and a contributor to the Open MPI project.
</p>

<center>
Sponsored by <a href="">Information Science & Technology Institute</a> (ISTI) 
</center>
LA-UR-24-30763
</body>
</html> 
