 <!DOCTYPE html>
<html>
<head>
<title>Advances in Applied Computer Science Invited Speaker Series</title>
<style>
body {
  font-family: 'Courier New', monospace;
}
a:link {
  color: white;
  background-color: transparent;
  text-decoration: none;
}
a:visited {
  color:white;
  background-color: transparent;
  text-decoration: none;
}
a:hover {
  color: white;
  background-color: transparent;
  text-decoration: underline;
}
a:active {
  color: white;
  background-color: transparent;
  text-decoration: underline;
}
li::marker {
  color: white;
}
</style>
</head>
<body style="background-color:black;color:white;">

<h1>Advances in Applied Computer Science Invited Speaker Series</h1>

In applied computer science new programming languages and paradigms, like Rust and Julia, new architectures, like Arm and RISC-V, and new GPU’s, like NVidia’s Grace Hopper, AMD's MI300A or Intel Max, are emerging. To keep on top of these advances, the Advances in Applied Computer Science series will invite speakers from academia, industry, and national labs. This will allow researchers and code developers to learn about these new tools and technologies and integrate them into codes to prepare them for future hardware and software.
<p>
Host: Patrick Diehl<br>
Co-Host: Philipp Edelmann<br>

<h2>Overview</h2>

<table style="width:90%">
  <tr>
      <td><b>Speaker</b></td>
      <td><b>Title</b></td>
      <td><b>Date</b></td>
  </tr>
  <tr>
      <td>Dominic Marcello</td>
      <td><a href="#fy262">Shedding Light on Interaction Binaries: Radiation Hydrodynamics with Octo-Tiger</a></td>
      <td>07/25/2025</td>
  </tr>
  <tr>
      <td>Jiakun Yan</td>
      <td><a href="#fy258">LCI: a Lightweight Communication Interface for Asynchronous Multithreaded Communication</a></td>
      <td>05/27/2025</td>
  </tr>
  <tr>
      <td>Aparna Chandramowlishwaran</td>
      <td><a href="#fy261">Scaling AI for Scientific Discovery: Faster Kernels, Efficient Models, Better Physics</a></td>
      <td>04/15/2025</td>
  </tr>
  <tr>
      <td>Richard Berger</td>
      <td><a href="#fy260">Driving Continuous Integration and Developer Workflows with Spack</a></td>
      <td>04/08/2025</td>
  </tr>
  <tr>
      <td>Ryan Friese</td>
      <td><a href="#fy259">Introducing the Lamellar Runtime: A Modern Approach to High-Performance Computing</a></td>
      <td>04/02/2025</td>
  </tr>
  <tr>
      <td>Johannes Blaschke</td>
      <td><a href="#fy257">Deploying and Supporting Julia at NERSC</a></td>
      <td>03/12/2025</td>
  </tr>
  <tr>
      <td>William Godoy</td>
      <td><a href="#fy256">The value proposition of the Julia language for DOE’s mission</a></td>
      <td>03/12/2025</td>
  </tr>
  <tr>
      <td>Jonas Posner</td>
      <td><a href="#fy255">Transparent Resource Adaptivity for Task-Based Applications on Supercomputers</a></td>
      <td>02/24/2025</td>
  </tr>
  <tr>
      <td>Jed Brown</td>
      <td><a href="#fy254">Rust and type safety for MPI and scientific computing</a></td>
      <td>02/12/2025</td>
  </tr>
  <tr>
      <td>Alexander Holas</td>
      <td><a href="#fy253">Thermonuclear electron-capture supernovae - Motivating a long-overdue update to the supernova modeling pipeline for the exascale computing age</a></td>
      <td>01/09/2025</td>
  </tr>
  <tr>
      <td>Gregor Dai&szlig;</td>
      <td><a href="#fy252">Asynchronous-Many-Task Systems: Challenges and Opportunities - Scaling an AMR Astrophysics Code on Exascale machines using Kokkos and HPX</a></td>
      <td>12/11/2024</td>
  </tr>
  <tr>
      <td>Chris Taylor</td>
      <td><a href="#fy251">RISC-V HPC Terrain Familiarization</a></td>
      <td>12/04/2024</td>
  </tr>
   <tr>
       <td></td>
       <td><b>Fiscal year 2024</b></td>
       <td></td>
  </tr>
  <tr>
      <td>Kyle Niemeyer</td>
      <td><a href="#fy242">Journal of Open Source Software: bot-assisted open peer review and publication</a></td>
      <td>09/19/2024</td>
  </tr>
  <tr>
      <td>Joseph Schuchart</td>
      <td><a href="#fy241">Improving MPI Interoperability for Modern Languages and Systems</a></td>
      <td>08/21/2024</td>
  </tr>
</table>

<h2>Talks</h2>




<div id="fy258"></div>
<h3>LCI: a Lightweight Communication Interface for Asynchronous Multithreaded Communication</h3>
Speaker: Jiakun Yan<br>
University of Illinois Urbana-Champaign<br><br>

Abstract:
<p>
The Lightweight Communication Interface (LCI) is an experimental communication library aiming for better asynchronous multithreaded communication support, both in terms of performance and programmability. It is also a research tool helping us understand how to design communication libraries to better fit the needs of dynamic programming systems/applications. It features a simple, incrementally refinable interface unifying all common point-to-point communication primitives and an atomic-based runtime for maximum threading efficiency. It has been integrated into established asynchronous many-task systems such as HPX and shown significant performance improvement on microbenchmarks/real-world applications. This talk will present an overview of its interface and software design and showcase its performance.
</p>

Bio:
<p>
Jiakun Yan is a fifth-year Ph.D. student at UIUC, advised by Prof. Marc Snir. His research involves exploring better communication library designs for highly dynamic/irregular programming systems and applications. He is the main contributor to the Lightweight Communication Interface (LCI) Project and the HPX LCI parcelport.
</p>


<div id="fy261"></div>
<h3>Scaling AI for Scientific Discovery: Faster Kernels, Efficient Models, Better Physics</h3>
Speaker: Aparna Chandramowlishwaran<br>
University of California, Irvine<br><br>

Abstract:
<p>
As AI becomes a powerful tool in scientific computing, two challenges emerge: (1) efficiently scaling models on modern hardware, and (2) ensuring these models can learn complex physics with fidelity and generalizability. In this talk, I will discuss our work at this intersection.
First, I will introduce Fused3S, a GPU-optimized algorithm for sparse attention—the backbone of graph neural networks and transformers. By fusing matrix operations, Fused3S reduces data movement and maximizes tensor core utilization, achieving state-of-the-art performance across diverse workloads. Then, I will present BubbleML and Bubbleformer, our efforts to model boiling dynamics with ML. Boiling is fundamental to energy, aerospace, and nuclear applications, yet remains difficult to model due to the interplay of turbulence, phase change, and nucleation. By combining large-scale simulation datasets with transformer architectures, Bubbleformer forecasts boiling dynamics across fluids, geometries, and operating conditions. Together, these efforts illustrate how scaling AI—both computationally and scientifically—can accelerate discovery across disciplines. I will conclude with open challenges and opportunities in AI-driven scientific computing, from hardware-aware models to foundation models for physics.
</p>

Bio:
<p>
Aparna Chandramowlishwaran is an Associate Professor at the University of California, Irvine, in the Department of Electrical Engineering and Computer Science. She received her Ph.D. in Computational Science and Engineering from Georgia Tech in 2013 and was a research scientist at MIT prior to joining UCI as an Assistant Professor in 2015. Her research lab— HPC Forge—aims at advancing computational science using high-performance computing and machine learning. She currently serves as the associate editor of the ACM Transactions on AI for Science.
</p>


<div id="fy260"></div>
<h3>Driving Continuous Integration and Developer Workflows with Spack</h3>
Speaker: Richard Berger<br>
Los Alamos National Laboratory<br><br>

Abstract:
<p>
Spack makes it easy to install dependencies for our software on multiple HPC platforms. However, there is little guidance on how to structure Spack environments for larger projects, share common Spack installations with code teams and utilize them in an effective way for continuous integration and development. This presentation will share some of the lessons learned from deploying chained Spack installations for multiple code teams at LANL on various HPC platforms both on site and on other Tri-Lab systems, how to structure such deployments for reusability and upgradability, and make them deployable even on air-gapped systems. It will also show how we utilize Spack's build facilities to drive CMake-based projects on GitLab for continuous integration, without having to replicate build configuration logic in GitLab files, while giving developers an easy-to-follow workflow for recreating CI runs in various configurations.
</p>

Bio:
<p>
Richard is a research software engineer in the Applied Computer Science Group (CCS-7) at Los Alamos National Laboratory (LANL) with a background in Mechatronics, high-performance computing, and software engineering. He is currently contributing to the core development of LAMMPS, FleCSI and working on DevOps for multiple other LANL projects.
</p>

<div id="fy259"></div>
<h3>Introducing the Lamellar Runtime: A Modern Approach to High-Performance Computing</h3>
Speaker: Ryan Friese<br>
Pacific Northwest National Laboratory<br>
Senior computer scientist<br><br>

Abstract:
<p>
In the realm of High-Performance Computing (HPC), achieving peak system performance while maintaining code safety and concurrency has always been a challenging endeavor. Traditional HPC frameworks often struggle with memory safety issues, race conditions, and the complexities of parallel programming. In this talk, I introduce Lamellar, a new HPC runtime that leverages the modern programming language Rust to address these enduring challenges.
Rust, known for its powerful type system and memory safety guarantees without a garbage collector, is rapidly gaining traction within the systems programming community. However, its potential in the HPC domain is yet to be fully explored. The Lamellar runtime harnesses Rust's strengths, providing a robust, scalable, and safe environment for developing and executing high-performance applications.
This talk is designed for computational domain and computer scientists who are well-acquainted with HPC concepts but may be new to Rust. The talk will cover the following key topics:
<ol>
    <li>Introduction to Rust: An overview of Rust’s features and why it's suitable for HPC applications.</li>
    <li>Understanding the Lamellar Runtime: A deep dive into the architecture and core components of the Lamellar runtime and stack.</li>
    <li>Concurrency and Parallelism: How Rust’s concurrency model and Lamellar API’s ensure safe parallel execution, eliminating common pitfalls like data races.</li>
    <li>Performance Benchmarks: Comparative analysis showcasing the performance benefits of using Rust Lamellar Runtime in selected benchmarks.
    </li>
    <li>Getting Started: Resources, tools, and best practices for adopting Rust and Lamellar in your HPC projects.</li>
</ol>
Join me as we look into the future of HPC with Rust and the Lamellar Runtime.
<p>
Bio:
<p>
Dr. Ryan Friese is a senior computer scientist at Pacific Northwest National Laboratory in the Future Computing Technologies group. His research interests span hardware/software co-design of runtime and system software for novel architectures, HPC (high performance computing) network simulation and modeling, the analysis and optimization of data movement in large scale distributed workflows, and performance modeling of irregular applications. His recent work has focused on enabling memory safe programming on HPC systems by leading the development of the Lamellar Runtime, an asynchronous distributed runtime written in the Rust Programming Language. He received his PhD in Electrical & Computer Engineering in 2015 from Colorado State University. 
</p>


<div id="fy257"></div>
<h3>Deploying and Supporting Julia at NERSC</h3>
Speaker: Johannes Blaschke<br>
National Energy Research Scientific Computing Center<br>
Application Performance Specialist<br><br>

Abstract:
<p>
Julia has been supported as a first-class language on NERSC's systems for over 10 years [1]. In this talk we will discuss how Julia has been deployed at scale, the technical issues encountered and how they were eventually overcome. We will also explore the challenges faced with developing intuitive and portable distributed HPC applications that are capable of targeting modern GPU architectures, and NERSC's vision for supporting modern interdisciplinary and multi-facility workflows. 
<br>
<br>
[1] <a href="https://info.juliahub.com/case-studies/celest">https://info.juliahub.com/case-studies/celeste</a>
</p>

Bio:
<p>
Johannes Blaschke is a HPC workflow performance expert leading the NERSC Science Acceleration Program (NESAP). His research interests include urgent and interactive HPC; and programming environments and models for cross-facility workflows. Johannes supports Julia on NERSC's systems, including one of the first examples of integrating MPI.jl and Distributed.jl with the HPE's Slingshot network technology. Johannes is a zealous advocate for Julia as an HPC programming language, and a contributor and organizer of Julia tutorials and BoFs at SC, JuliaCon and within the DoE.
</p>

<div id="fy256"></div>
<h3>The value proposition of the Julia language for DOE’s mission</h3>
Speaker: William Godoy<br>
Oak Ridge National Laboratory<br>
Senior computer scientist<br><br>

Abstract:
<p>
We present a summary of our research and community efforts exploring the Julia
language for the scientific mission of the US Department of Energy (DOE) at the
intersection of high-performance computing (HPC) and high-productivity. Powered by
the LLVM compiler infrastructure combined with a unifying ecosystem and friendly
scientific syntax, Julia attempts to lower cost of a “two-language and multiple
ecosystems” paradigm (e.g. Python+compiled language). Along with the Julia intro and
HPC hands-on tutorials, we present our efforts on: (i) building an accessible
performance portable CPU/GPU library: JACC.jl, (ii) the outcome of external venues
(SC BoFs, tutorials) and workshops at Oak Ridge National Laboratory (ORNL), and (iii)
our research, best paper at SC23 WORKS, on the unifying value for using a single
front-end language on Frontier, the second fastest supercomputer in the world, and (iv)
our work, best paper at SC24 XLOOP, connecting ORNL’s experimental and
computational facilities using JACC.jl. Hence, Julia aspires to make more accessible the
future landscape of heterogeneous, AI-driven, and energy-aware computing by
leveraging existing investments outside DOE in LLVM and commercial applications of
the language.
</p>

Bio:
<p>
William Godoy is a senior computer scientist in the Computer Science and Mathematics
Division at Oak Ridge National Laboratory (ORNL). His interests are in
high-performance computing, parallel programming systems, scientific software and
workflows. At ORNL, he contributed to the Exascale Computing Project applications
-QMCPACK- and software technologies portfolios – ADIOS2, Julia/LLVM, and projects
impacting ORNL’s computing and neutron science facilities. Godoy currently works
across research projects funded by the US Department of Energy Advanced Scientific
Computing Research (ASCR) program. Prior to ORNL, he was a staff member at Intel
Corporation and a postdoctoral fellow at NASA Langley Research Center. Godoy
received PhD and MSc degrees from the University at Buffalo, The State University of
New York, and a BSc from the National Engineering University (UNI) Lima, Peru, in
mechanical engineering. He is a senior member of the IEEE, and a member of ACM,
ASME and US-RSE serving in several venues and technical committees.
</p>

<div id="fy255"></div>
<h3>Transparent Resource Adaptivity for Task-Based Applications on Supercomputers</h3>
Speaker: Jonas Posner<br>
University of Kassel, Germany<br>
Substitute Chair - Software Engineering<br><br>

Abstract:
<p>
Traditional static resource allocation in supercomputers (jobs retain a fixed set of resources) leads to inefficiencies. Resource adaptivity (jobs can change resources at runtime) significantly increases supercomputer efficiency.<br>
This talk will exploit Asynchronous Many-Task (AMT) programming, which is well suited for adaptivity due to its transparent resource management. An AMT runtime system dynamically assigns user-defined small tasks to workers to achieve load balancing and adapt to resource changes.<br>
We will discuss techniques for malleability and evolving capabilities that allow programs to dynamically change resources without interrupting computation. Automatic load detection heuristics determine when to start or terminate processes, which is particularly beneficial for unpredictable workloads. Practicality is demonstrated by adapting the GLB library. A generic communication interface allows interaction between programs and resource managers. Evaluations with a prototype resource manager show significant improvements in batch makespan, node utilization, and job turnaround time for both malleable and evolving jobs.
</p>

Bio:
<p>
Jonas is a dedicated computer scientist specializing in High Performance Computing. He received his Bachelor’s and Master’s degrees from the University of Kassel, Germany, where he also earned his Ph.D. in 2022. He is currently working as an substitute chair for the Software Engineering Group at the same university and is also writing his habilitation.<br>
Jonas' research interests include load balancing, fault tolerance, and resource adaptivity for Asynchronous Many-Task (AMT) systems. Recently, he has focused on resource adaptivity in general to optimize the efficient use of supercomputing resources. His work covers a broad spectrum, including the development of advanced job scheduling algorithms, the improvement of application programming using AMT systems, and the interaction between resource managers and jobs.
</p>

<div id="fy254" />
<h3>Rust and type safety for MPI and scientific computing</h3>
Speaker: Jed Brown<br>
University of Colorado Boulder<br>
Associate Professor of Computer Science<br><br>

Abstract:
<p>
Rust is a modern language that provides type- and memory-safety without a garbage collector, using a concept called lifetimes. Many see Rust as a successor to languages like C and C++, and there are many interested individuals in the computational science community, yet few major projects have made the switch. I'll introduce the language and its ecosystem, including the state of scientific computing libraries. We'll discuss what soundness means for libraries and examine rsmpi, which safely exposes MPI and allows catching many common bugs at compile-time. We'll also discuss type-system approaches to collective semantics, and conclude with an outlook on Rust for scientific computing.
</p>

Bio:
<p>
Jed leads the Physical Prediction, Inference, and Design group at CU Boulder. He is a maintainer of PETSc, libCEED, and rsmpi (Rust bindings to MPI), and is active in many open source communities. He works on high-performance numerical software infrastructure for computational science and engineering, as well as applications such as structural mechanics and materials science, non-Newtonian and turbulent flows, and plasmas. He is co-director of the PSAAP-3 Multidisciplinary Simulation Center for Micromorphic Multiphysics Porous and Particulate Materials Simulations Within Exascale Computing Workflows.
</p>


<div id="fy253"></div>
<h3>Thermonuclear electron-capture supernovae - Motivating a long-overdue update to the supernova modeling pipeline for the exascale computing age</h3>
Speaker: Alexander Holas<br>
Heidelberg Institute for Theoretical Studies<br><br>

Abstract:
<p>
The thermonuclear supernova modeling pipeline has been refined for over four decades and has achieved substantial success in modeling various supernova subtypes. Nonetheless, continuous innovation is essential for maintaining supernova modeling at the forefront of computational astrophysics. In this work, we examine a novel scenario, so called thermonuclear electron-capture supernovae. Originally proposed by Jones et al. (2016), this scenario consists of a collapsing sAGB star that only narrowly escape collapse to a neutron star by runaway thermonuclear thermonuclear burning. Here, we explore the specific circumstances under which such a thermonuclear explosion can occur and under which conditions the collapse can be averted by nuclear burning. Subsequently, we leverage this scenario to motivate a long-overdue update to the thermonuclear supernova modeling pipeline, both by increasing the complexity of the physics included, as well as updating the underlying codebase for the latest exascale computing clusters. In particular, we advocate the integration of radiation hydrodynamics and the transition towards a performance portable programming model.
</P>

Bio:
<ul>
<li> Undergraduate at Ulm University</li>
<li> Master Studies at the Technical University of Munich and the Max Planck Institute for Astrophysics (Thesis: Determination of the Expansion Rate of the Universe by Means of Type II‑P Supernovae)</li>
<li> PhD at the Heidelberg Institute for Theoretical Studies/ Heidelberg University on numerical simulations of thermonuclear supernovae (Working project title: Thermonuclear electron-capture supernovae - thermonuclear explosion or gravitational collapse?)</li>
<li> Fellow at the International Max Planck Research School Heidelberg</li>
</ul>

<div id="fy251"></div>
<h3>RISC-V HPC Terrain Familiarization</h3>
Speaker: Chris Taylor<br>
Tactical Computing Lab<br>
Principle Research Engineer<br><br>

Abstract:
<p>
The number of RISC-V commercial products increased substantially this past year. This presentation is an orientation to the range of RISC-V hardware, HPC software support, the community, and the current state of HPC-relevant ISA extensions. Acquiring RISC-V hardware is no longer a question of when - it is possible now.
</p>

Bio:
<p>
Chris is a senior principle research engineer at Tactical Computing Labs. His work experience includes compilers, runtime systems, systems level software, numerical libraries, applied math problems, and hardware simulation. He has a Masters Degree in Computer Science from Georgia Tech and an undergraduate degree in Computer Science from Clemson.
</p>

<div id="fy252"></div>
<h3>Asynchronous-Many-Task Systems: Challenges and Opportunities - Scaling an AMR Astrophysics Code on Exascale machines using Kokkos and HPX</h3>

Speaker: Gregor Dai&szlig;<br>
University of Stuttgart<br>
Institute for Parallel and Distributed System<br><br>

Abstract:
<p>
Dynamic and adaptive mesh refinement is pivotal in high-resolution,
multi-physics simulations, necessitating precise physics resolution in
localized areas across expansive domains. Today's supercomputers'
extreme heterogeneity and large number of compute nodes present a
significant challenge for such dynamically adaptive codes, highlighting
the importance of both scalability and performance portability. Our
research focuses on how to address this by integrating the asynchronous,
many-task runtime system HPX with the performance-portability framework
Kokkos and SIMD types. To demonstrate and benchmark our solutions at
scale, we incorporated them into Octo-Tiger, an adaptive, massively
parallel application for the simulation of binary star systems and their
outcomes. Thanks to this, Octo-Tiger now supports a diverse set of
processors, accelerators, as well as network backends and can scale on
various supercomputers, such as Perlmutter, Frontier, and Fugaku. In
this talk, we outline our various integrations between HPX and Kokkos.
Furthermore, we show the challenges we encountered when using these
frameworks together in Octo-Tiger and how we addressed them, ultimately
achieving scalability on a selection of current supercomputers.
</p>

Bio:
<p>
Gregor Daiß is a PhD student at the University of Stuttgart,
specializing in high-performance computing. His main interests
include task-based runtime systems, distributed computing,
performance-portability as well as refactoring large-scale
simulations and porting them to accelerators. Current work
mostly involves both Kokkos (for performance-portability) and
HPX (task-based runtime system) for these purposes.
</p>

<div id="fy242"></div>
<h3>Journal of Open Source Software: bot-assisted open peer review and publication</h3>

Speaker: Kyle Niemeyer<br>
Oregon State University<br>
School of Mechanical, Industrial, and Manufacturing Engineering<br>
Associate Professor<br><br>

Abstract:
<p>
The Journal of Open Source Software (JOSS) is an open-access, no-fee scholarly journal that publishes quality open-source research software based on open peer review. JOSS was founded in 2016 with the dual objectives of giving traditional academic publication credit for software work and improving the quality of research software. Since its founding, JOSS has published over 2500 software papers—and counting!—with over 80 active editors spread across seven topic-area tracks. To handle this level of submissions and publishing, relying on a fully volunteer team, JOSS relies on GitHub and a system of open tools for reviewing and publishing submissions, driven by chatbot commands. Authors submit short Markdown papers along with links to their software's repository, which are compiled to PDF via Pandoc. JOSS’s editorial bot performs automated health checks on submissions, and reviews take place in GitHub issues, with authors, editors, and reviewers issuing bot commands via comments. This talk will describe the publication experience of JOSS and its machinery, and how it can be adapted by other communities.
</p>

Bio:
<p>
Kyle E. Niemeyer is an Associate Professor at Oregon State University in the School of Mechanical, Industrial, and Manufacturing Engineering. He also serves as the Associate School Head for Undergraduate Programs. He leads the Niemeyer Research Group, which uses computational modeling to study various phenomena involving fluid flows, including combustion and chemical kinetics, and related topics like numerical methods and parallel computing. He is also a strong advocate of open access, open source software, and open science in general, and has contributed in the area of standardizing research software citation. Kyle has received multiple prestigious fellowships throughout his career, including the AAAS Science & Technology Policy Fellowship in 2022, the Better Scientific Software (BSSw) Fellowship in 2019, the NSF Graduate Research Fellowship in 2010, and the National Defense Science and Engineering Graduate Fellowship in 2009. Kyle received his Ph.D. in Mechanical Engineering from Case Western Reserve University in 2013. He received BS and MS degrees in Aerospace Engineering from Case Western Reserve University in 2009 and 2010, respectively.
</p>
Material:
<ul>
    <li> <a href="https://zenodo.org/records/13799976">Slides</a></li>
</ul>

<div id="fy241"></div>
<h3>Improving MPI Interoperability for Modern Languages and Systems</h3>

Speaker: Joseph Schuchart<br>
Stony Brook University<br>
Institute for Advanced Computational Science<br>
Senior Research Scientist<br><br>

Abstract:
<p>
The Message Passing Interface standard has long been the lingua franca of HPC. Its design has enabled the development of many distributed parallel applications. After 30 years, the field of high-performance computing has seen several programming paradigms come and go. However, MPI has yet to address the challenges of accelerator-based computing, the advent of modern languages such as Rust, Python, and C++, and fully asynchronous programming models. This talk will provide insights into current efforts on modernizing MPI, from accelerator integration to improved datatype handling for modern languages.
</p>

Bio:
<p>
Joseph Schuchart is a Senior Research Scientist at the Institute for Advanced Computational Sciences at Stony Brook University. His research revolves around distributed asynchronous and task-based programming models, communication libraries, and design aspects of integrating different models. Joseph received his M.Sc. in Computer Science from Dresden University of Technology in 2012 and his PhD from the University of Stuttgart in 2020. He is an active member of the MPI Forum and a contributor to the Open MPI project.
</p>

<center>
Sponsored by <a href="">Information Science & Technology Institute</a> (ISTI) 
</center>
LA-UR-24-30763
</body>
</html> 
